"use strict";(self.webpackChunk_openfn_docs=self.webpackChunk_openfn_docs||[]).push([[38012],{28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var t=i(96540);const s={},o=t.createContext(s);function a(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(o.Provider,{value:n},e.children)}},34062:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});var t=i(71924),s=i(74848),o=i(28453);const a={layout:"post",title:"Improving Multistage Docker Builds using Buildx",authors:"stu",tags:["how-to","docker","ci/cd"],featured:!0},r=void 0,l={authorsImageUrls:[void 0]},c=[{value:"Buildx",id:"buildx",level:2},{value:"Local Cache",id:"local-cache",level:3},{value:"Remote Cache",id:"remote-cache",level:3},{value:"Tips",id:"tips",level:2},{value:"Closing thoughts",id:"closing-thoughts",level:2},{value:"Resources",id:"resources",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",br:"br",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"So you're using docker's multi-stage builds and noticed that your build times\naren't nearly as quick as you expected?"}),"\n",(0,s.jsx)(n.p,{children:"As many teams who spend more and more time using docker, it's quite common to\nget into multi-stage builds; usually resulting in significantly smaller images."}),"\n",(0,s.jsxs)(n.p,{children:["However this comes with a pretty significant dilemma with caching. Even when\nusing the ",(0,s.jsx)(n.code,{children:"--cache-from"})," flag when building, docker only caches the last image."]}),"\n",(0,s.jsxs)(n.p,{children:["One proposed solution",(0,s.jsx)("sup",{children:(0,s.jsx)(n.a,{href:"#ref1",children:"1"})}),", is to pull, build and push each\nindividual stage. Coming with tight coupling between the shape of your\nDockerfile and your build process/scripts."]}),"\n",(0,s.jsx)(n.p,{children:"The other solution uses Docker Buildx which the document describes as:"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"Docker Buildx is a CLI plugin that extends the docker command with the full\nsupport of the features provided by Moby BuildKit builder toolkit. It provides\nthe same user experience as docker build with many new features like creating\nscoped builder instances and building against multiple nodes concurrently."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"While that sounds pretty cool, it doesn't really touch on caching. This actually\ntook me a while to find out that it would in fact do caching very differently.\nIn fact it's a very different experience using it, and has lots of really cool\nfeatures that further detach you from the local docker state allowing you to\nbuild in environments that are stateless - such as Google CloudBuild without\nhaving to wire up some kind of persistence or file caching scheme."}),"\n",(0,s.jsx)(n.h2,{id:"buildx",children:"Buildx"}),"\n",(0,s.jsx)(n.p,{children:"We're only going to scratch the surface of Buildx, and with that let's get the\nabsolute minimum working; build our image locally."}),"\n",(0,s.jsx)(n.h3,{id:"local-cache",children:"Local Cache"}),"\n",(0,s.jsxs)(n.p,{children:["First things first we need to create a builder, and select it for use. This is\nimportant as without creating a buildx builder (and setting it as the default),\nbuildx will use the ",(0,s.jsx)(n.code,{children:"docker"})," driver instead of the ",(0,s.jsx)(n.code,{children:"docker-container"})," driver\nwhich we want in order to take advantage of cache exporting."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"docker buildx create --name mybuilder --use\n"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"You only need to run this once, except in the case of CloudBuild where each\ninvocation is a new node."}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"docker buildx build \\\n  --cache-from=type=local,src=/tmp/buildx-cache \\\n  --cache-to=type=local,dest=/tmp/buildx-cache \\\n  --load \\\n  .\n"})}),"\n",(0,s.jsxs)(n.p,{children:["While the ",(0,s.jsx)(n.code,{children:"--cache-*"})," options aren't specifically required when running ",(0,s.jsx)(n.code,{children:"build"}),",\nas ",(0,s.jsx)(n.code,{children:"buildx"})," does manage its own local cache (distinct from the regular docker\ncache), it's there to emphasise the options that cache can be provided via the\nCLI options."]}),"\n",(0,s.jsx)(n.p,{children:"This is about as close as you get to a regular docker build, with the\nsignificant difference being that you have to specify where to cache from and\nto."}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"--load"})," flag is to tell buildx to set the output to the local docker\ndaemon. Without that you won't actually get a resulting image to run. However,\ndepending on your use case, this could be seen as a convenience - if you're\nwanting to run your tests inside your build; a resulting image isn't\nparticularly useful."]}),"\n",(0,s.jsx)(n.h3,{id:"remote-cache",children:"Remote Cache"}),"\n",(0,s.jsx)(n.p,{children:"Now comes to the part I'm most interested in, caching in a stateless/remote\nenvironment. Multipart builds for us at OpenFn are essential, since we use\nElixir and like other compiled languages there is a lot to be gained by only\nshipping the stuff you're going to run; and no language is safe from requiring\nseveral times more 'stuff' in order to build our apps."}),"\n",(0,s.jsxs)(n.p,{children:["Buildx supports a\n",(0,s.jsx)(n.a,{href:"https://github.com/docker/buildx/blob/master/docs/reference/buildx_build.md#-export-build-cache-to-an-external-cache-destination---cache-to",children:"handful of different types"}),"\nof caching sources and destinations. We're going to be using the ",(0,s.jsx)(n.code,{children:"registry"}),"\ntype, where you point the cache at a repository reference (repo/image",":tag","\nstyle)."]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"One thing to note is that Google Container Registry does not support the\nmetadata/manifest format that buildx uses, so if you're using Google Cloud you\nwill need to start using Artifact Registry."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Inline"})}),"\n",(0,s.jsx)(n.p,{children:"Push the image and the cache together:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"...\n--cache-from=type=registry,ref=$IMAGE_NAME \\\n--cache-to=type=inline \\\n...\n"})}),"\n",(0,s.jsxs)(n.p,{children:["This comes with the constraint that cache mode is always ",(0,s.jsx)(n.code,{children:"min"}),", which only\nexports/caches the resulting layers; which is still better than the plain docker\nbuild caching but I think having the intermediary layers is generally a win. We\nwant to avoid a single line change invalidating an entire build step."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Registry"})}),"\n",(0,s.jsx)(n.p,{children:"Resulting image and cache are separated:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"...\n--cache-from=type=registry,ref=$IMAGE_NAME-build-cache \\\n--cache-to=type=registry,ref=$IMAGE_NAME-build-cache,mode=max \\\n...\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Again coming back to the cache mode, here being ",(0,s.jsx)(n.code,{children:"max"}),"; all intermediary laters\nare exported to the cache image as well."]}),"\n",(0,s.jsxs)(n.p,{children:["I have opted to create ",(0,s.jsx)(n.em,{children:"two"})," images, one for caching and another for the\nresulting image used to deploy. This gains us a much more granular cache and the\nability to more easily manage the cache image - like deleting the whole thing\nwhen wanting to invalidate the cache. Not to mention I'm fairly sure the size of\nour images that get pulled on kubernetes would get significantly larger with\nmany more layers."]}),"\n",(0,s.jsx)(n.p,{children:"It feels like a safer bet to have lean images for kubernetes to pull, and chunky\ncache images specifically for speeding up build."}),"\n",(0,s.jsxs)(n.p,{children:["Depending on your setup, pulling large images can get ",(0,s.jsx)(n.em,{children:"seriously"})," expensive in a\nreasonably active deployment environment - like on AWS ECS without using\nPrivateLink."]}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:["It appears the ",(0,s.jsx)(n.code,{children:"moby/buildkit"})," documentation also demonstrates\n",(0,s.jsx)(n.a,{href:"https://github.com/moby/buildkit#registry-push-image-and-cache-separately",children:"this"}),"\napproach."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"IMAGE_NAME=us-east4-docker.pkg.dev/<project-name>/platform/app \\\ndocker buildx build \\\n  -t $IMAGE_NAME:latest \\\n  --cache-from=type=registry,ref=$IMAGE_NAME-build-cache \\\n  --cache-to=type=registry,ref=$IMAGE_NAME-build-cache,mode=max \\\n  --push \\\n  --progress=plain \\\n  .\n"})}),"\n",(0,s.jsxs)(n.p,{children:["This implies that the cache image is named with the suffix ",(0,s.jsx)(n.code,{children:"-build-cache"}),":",(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.code,{children:"us-east4-docker.pkg.dev/<project-name>/platform/app[-build-cache]"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"--push"})," argument tells buildx to push the resulting image to the registry."]}),"\n",(0,s.jsx)(n.h2,{id:"tips",children:"Tips"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Clearing the local cache"})}),"\n",(0,s.jsxs)(n.p,{children:["As mentioned before, buildx has its own cache and in order to clear the cache\nwhile debugging and readying a Dockerfile for remote building you'll probably\nneed to reach for ",(0,s.jsx)(n.code,{children:"docker buildx prune"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"closing-thoughts",children:"Closing thoughts"}),"\n",(0,s.jsx)(n.p,{children:"Using buildx has been a really pleasant experience, having personally attempted\nusing it a few times over the last 3 years; the most recent one being the first\ntime I felt confident getting it into production. As with any sufficiently\nflexible build tooling, the errors and issues you can run into range from\ncomplete gibberish, genuinely concerning inconsistencies to architectural\nchoices that you haven't fully caught up on; requiring an ever growing list of\nchanges you need to make to your own build process."}),"\n",(0,s.jsx)(n.p,{children:"Our initial observations have been great, reasonable changes on our build have\ngone from 28 minutes to around 9 minutes."}),"\n",(0,s.jsxs)(n.p,{children:["While I have encountered a few confusing cache invalidations, especially when\nbuilding locally, exporting the cache to a repository and then having CloudBuild\nuse the image cache. And occasionally locally having what feels like ",(0,s.jsx)(n.em,{children:"really"}),"\naggressive caching on intermediate steps, leading me to pruning the local cache."]}),"\n",(0,s.jsx)(n.p,{children:"But overall, these issues aren't necessarily buildx issues and more likely a\ncombination of building docker images in general except with many more steps\naccounted for by the cache."}),"\n",(0,s.jsx)(n.p,{children:"It's kinda hard to see now what the exact issues I had with it in the past, but\nhey!"}),"\n",(0,s.jsx)(n.p,{children:"Buildx has given me what I 'expected' with docker multi-stage builds, and having\nthe cache in a repository completely side-steps having to attach a shared volume\nor copying from a storage bucket."}),"\n",(0,s.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://pythonspeed.com/articles/faster-multi-stage-builds/",children:"Multi-stage builds #3: Speeding up your builds"}),"\n",(0,s.jsx)("a",{name:"ref1",children:(0,s.jsx)("sup",{children:"1"})})]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.docker.com/buildx/working-with-buildx/",children:"Docker Buildx"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/docker/buildx/blob/master/docs/reference/buildx_build.md#buildx-build",children:"buildx build reference"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/moby/buildkit#registry-push-image-and-cache-separately",children:"mody/buildkey Registry cache exporter"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},71924:e=>{e.exports=JSON.parse('{"permalink":"/articles/2021/10/08/improving-multistage-docker-builds-using-buildx","editUrl":"https://github.com/openfn/docs/edit/main/articles/2021-10-08-improving-multistage-docker-builds-using-buildx.md","source":"@site/articles/2021-10-08-improving-multistage-docker-builds-using-buildx.md","title":"Improving Multistage Docker Builds using Buildx","description":"So you\'re using docker\'s multi-stage builds and noticed that your build times","date":"2021-10-08T00:00:00.000Z","tags":[{"inline":true,"label":"how-to","permalink":"/articles/tags/how-to"},{"inline":true,"label":"docker","permalink":"/articles/tags/docker"},{"inline":true,"label":"ci/cd","permalink":"/articles/tags/ci-cd"}],"readingTime":6.49,"hasTruncateMarker":true,"authors":[{"name":"Stuart Corbishley","socials":{"github":"https://github.com/stuartc"},"imageURL":"https://avatars.githubusercontent.com/stuartc","key":"stu","page":null}],"frontMatter":{"layout":"post","title":"Improving Multistage Docker Builds using Buildx","authors":"stu","tags":["how-to","docker","ci/cd"],"featured":true},"unlisted":false,"prevItem":{"title":"Moving from Webpack to esbuild on Phoenix","permalink":"/articles/2021/10/15/webpack-to-esbuild-part1"},"nextItem":{"title":"Wrapping my head around jobs","permalink":"/articles/2021/07/05/wrapping-my-head-around-jobs"}}')}}]);